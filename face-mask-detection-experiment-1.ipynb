{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing the needed Libraries & Directories"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing the required libraries and directories are important to avoid errors for it also allows the codes to work perfectly."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:31.820073Z","iopub.status.busy":"2022-11-10T18:05:31.819191Z","iopub.status.idle":"2022-11-10T18:05:39.182591Z","shell.execute_reply":"2022-11-10T18:05:39.181435Z","shell.execute_reply.started":"2022-11-10T18:05:31.819982Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import json\n","import os\n","import matplotlib.pyplot as plt\n","import random\n","import seaborn as sns\n","from keras.models import Sequential\n","from keras import optimizers\n","from keras import backend as K\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator\n","directory = \"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\"\n","image_directory = \"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\"\n","df = pd.read_csv(\"./face-mask-detection-dataset/train.csv\")\n","df_test = pd.read_csv(\"./face-mask-detection-dataset/submission.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing SSD pretrained weights (Caffe Face Detector Model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["SSD or Single Shot Multibox Detector is used for detecting specific object in a given image. This SSD that I'm going to use is already pre-trained, which means although it has aquired some knowledge about its task, it still need to be trained (which I'll do later in the last lines of code for this project). "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.18486Z","iopub.status.busy":"2022-11-10T18:05:39.18449Z","iopub.status.idle":"2022-11-10T18:05:39.392987Z","shell.execute_reply":"2022-11-10T18:05:39.391805Z","shell.execute_reply.started":"2022-11-10T18:05:39.184826Z"},"trusted":true},"outputs":[],"source":["cvNet = cv2.dnn.readNetFromCaffe('./caffe-face-detector-opencv-pretrained-model/architecture.txt','./caffe-face-detector-opencv-pretrained-model/weights.caffemodel')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Functions to be used"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is the JSON Function. This function retrieves the json file from the training dataset which contains the bounding box data."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.394941Z","iopub.status.busy":"2022-11-10T18:05:39.394585Z","iopub.status.idle":"2022-11-10T18:05:39.400317Z","shell.execute_reply":"2022-11-10T18:05:39.399244Z","shell.execute_reply.started":"2022-11-10T18:05:39.394904Z"},"trusted":true},"outputs":[],"source":["def getJSON(filePathandName):\n","    with open(filePathandName,'r') as f:\n","        return json.load(f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next function is the Gamma correction. When applied, the images will get extra bright. For instance,  gamma < 1 will cause the image to get darker, and gamma > 1 will cause the image to be brighter."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.403892Z","iopub.status.busy":"2022-11-10T18:05:39.402625Z","iopub.status.idle":"2022-11-10T18:05:39.413171Z","shell.execute_reply":"2022-11-10T18:05:39.412088Z","shell.execute_reply.started":"2022-11-10T18:05:39.403854Z"},"trusted":true},"outputs":[],"source":["def adjust_gamma(image, gamma=1.0):\n","    invGamma = 1.0 / gamma\n","    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n","    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Pre-processing the Data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Data pre-processing is used for converting a raw data into a clean dataset.\n","Lets look at the JSON data given for training:\n","\n","The Annotations Field contains all the information about the faces shown in a given image. There are different classnames but the only true classnames are face_with_mask and face_no_mask."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.415287Z","iopub.status.busy":"2022-11-10T18:05:39.414156Z","iopub.status.idle":"2022-11-10T18:06:04.153253Z","shell.execute_reply":"2022-11-10T18:06:04.152043Z","shell.execute_reply.started":"2022-11-10T18:05:39.41525Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'FileName': '2310.png',\n"," 'NumOfAnno': 3,\n"," 'Annotations': [{'isProtected': False,\n","   'ID': 81768340414106304,\n","   'BoundingBox': [213, 169, 325, 299],\n","   'classname': 'face_other_covering',\n","   'Confidence': 1,\n","   'Attributes': {}},\n","  {'isProtected': False,\n","   'ID': 613664631614999424,\n","   'BoundingBox': [171, 168, 346, 299],\n","   'classname': 'hijab_niqab',\n","   'Confidence': 1,\n","   'Attributes': {}},\n","  {'isProtected': False,\n","   'ID': 166231871773593440,\n","   'BoundingBox': [435, 183, 470, 223],\n","   'classname': 'face_no_mask',\n","   'Confidence': 1,\n","   'Attributes': {}}]}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["jsonfiles= []\n","for i in os.listdir(directory):\n","    jsonfiles.append(getJSON(os.path.join(directory,i)))\n","jsonfiles[0]"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:06:04.15565Z","iopub.status.busy":"2022-11-10T18:06:04.154997Z","iopub.status.idle":"2022-11-10T18:06:04.1912Z","shell.execute_reply":"2022-11-10T18:06:04.190419Z","shell.execute_reply.started":"2022-11-10T18:06:04.1556Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>y1</th>\n","      <th>y2</th>\n","      <th>classname</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2756.png</td>\n","      <td>69</td>\n","      <td>126</td>\n","      <td>294</td>\n","      <td>392</td>\n","      <td>face_with_mask</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2756.png</td>\n","      <td>505</td>\n","      <td>10</td>\n","      <td>723</td>\n","      <td>283</td>\n","      <td>face_with_mask</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2756.png</td>\n","      <td>75</td>\n","      <td>252</td>\n","      <td>264</td>\n","      <td>390</td>\n","      <td>mask_colorful</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2756.png</td>\n","      <td>521</td>\n","      <td>136</td>\n","      <td>711</td>\n","      <td>277</td>\n","      <td>mask_colorful</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6098.jpg</td>\n","      <td>360</td>\n","      <td>85</td>\n","      <td>728</td>\n","      <td>653</td>\n","      <td>face_no_mask</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       name   x1   x2   y1   y2       classname\n","0  2756.png   69  126  294  392  face_with_mask\n","1  2756.png  505   10  723  283  face_with_mask\n","2  2756.png   75  252  264  390   mask_colorful\n","3  2756.png  521  136  711  277   mask_colorful\n","4  6098.jpg  360   85  728  653    face_no_mask"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"./face-mask-detection-dataset/train.csv\")\n","df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The mask label and non-mask label are utilized to extract bounding box information from json files. The very purpose of the Training Process is to extract and save the label and the faces of the given image into the data list."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:06:04.193107Z","iopub.status.busy":"2022-11-10T18:06:04.192595Z","iopub.status.idle":"2022-11-10T18:08:18.652631Z","shell.execute_reply":"2022-11-10T18:08:18.65157Z","shell.execute_reply.started":"2022-11-10T18:06:04.193068Z"},"trusted":true},"outputs":[],"source":["data = []\n","img_size = 124\n","mask = ['face_with_mask']\n","non_mask = [\"face_no_mask\"]\n","labels={'mask':0,'without mask':1}\n","for i in df[\"name\"].unique():\n","    f = i+\".json\"\n","    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n","        if j[\"classname\"] in mask:\n","            x,y,w,h = j[\"BoundingBox\"]\n","            img = cv2.imread(os.path.join(image_directory,i),1)\n","            img = img[y:h,x:w]\n","            img = cv2.resize(img,(img_size,img_size))\n","            data.append([img,labels[\"mask\"]])\n","        if j[\"classname\"] in non_mask:\n","            x,y,w,h = j[\"BoundingBox\"]\n","            img = cv2.imread(os.path.join(image_directory,i),1)\n","            img = img[y:h,x:w]\n","            img = cv2.resize(img,(img_size,img_size))    \n","            data.append([img,labels[\"without mask\"]])\n","random.shuffle(data)    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Checking how many objects are available in the given data."]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.654365Z","iopub.status.busy":"2022-11-10T18:08:18.653954Z","iopub.status.idle":"2022-11-10T18:08:18.662006Z","shell.execute_reply":"2022-11-10T18:08:18.660901Z","shell.execute_reply.started":"2022-11-10T18:08:18.654325Z"},"trusted":true},"outputs":[{"data":{"text/plain":["5749"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["len(data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Base on the visualization below, the Number of Mask images is greater than the Number of Non-Mask images which means its an imbalanced dataset."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.664249Z","iopub.status.busy":"2022-11-10T18:08:18.663346Z","iopub.status.idle":"2022-11-10T18:08:18.915775Z","shell.execute_reply":"2022-11-10T18:08:18.914884Z","shell.execute_reply.started":"2022-11-10T18:08:18.664215Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<AxesSubplot:ylabel='count'>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["p = []\n","for face in data:\n","    if(face[1] == 0):\n","        p.append('Mask')\n","    else:\n","        p.append('Without Mask')\n","\n","\n","sns.countplot(x=p)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is used for getting the shape of the features in the face mask data."]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.919893Z","iopub.status.busy":"2022-11-10T18:08:18.91904Z","iopub.status.idle":"2022-11-10T18:08:18.92802Z","shell.execute_reply":"2022-11-10T18:08:18.926856Z","shell.execute_reply.started":"2022-11-10T18:08:18.919858Z"},"trusted":true},"outputs":[],"source":["X = []\n","Y = []\n","for features,label in data:\n","    X.append(features)\n","    Y.append(label)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.929811Z","iopub.status.busy":"2022-11-10T18:08:18.92941Z","iopub.status.idle":"2022-11-10T18:08:18.944337Z","shell.execute_reply":"2022-11-10T18:08:18.943131Z","shell.execute_reply.started":"2022-11-10T18:08:18.92974Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(124, 124, 3)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["X[0].shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is used for getting the labels in the face mask data."]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.946519Z","iopub.status.busy":"2022-11-10T18:08:18.94586Z","iopub.status.idle":"2022-11-10T18:08:19.979438Z","shell.execute_reply":"2022-11-10T18:08:19.978212Z","shell.execute_reply.started":"2022-11-10T18:08:18.946478Z"},"trusted":true},"outputs":[],"source":["X = np.array(X)/255.0\n","X = X.reshape(-1,124,124,3)\n","Y = np.array(Y)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:19.98116Z","iopub.status.busy":"2022-11-10T18:08:19.980815Z","iopub.status.idle":"2022-11-10T18:08:19.988242Z","shell.execute_reply":"2022-11-10T18:08:19.98718Z","shell.execute_reply.started":"2022-11-10T18:08:19.981129Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(Y)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:19.990381Z","iopub.status.busy":"2022-11-10T18:08:19.989825Z","iopub.status.idle":"2022-11-10T18:08:20.003822Z","shell.execute_reply":"2022-11-10T18:08:20.002505Z","shell.execute_reply.started":"2022-11-10T18:08:19.990349Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5749,)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["Y.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Understanding the Model's Architecture and Training Process"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The project's model architecture is only a rough abstraction. To start the training process we have to set the model to sequential first, after that we can start adding the needed attributes to the model."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.006541Z","iopub.status.busy":"2022-11-10T18:08:20.005541Z","iopub.status.idle":"2022-11-10T18:08:20.315552Z","shell.execute_reply":"2022-11-10T18:08:20.31455Z","shell.execute_reply.started":"2022-11-10T18:08:20.006509Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-20 22:42:35.123204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-20 22:42:35.124616: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"]}],"source":["model = Sequential()\n","\n","model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n"," \n","model.add(Flatten())\n","model.add(Dropout(0.5))\n","model.add(Dense(50, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The line code below summarizes the model"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.317122Z","iopub.status.busy":"2022-11-10T18:08:20.316755Z","iopub.status.idle":"2022-11-10T18:08:20.3254Z","shell.execute_reply":"2022-11-10T18:08:20.323692Z","shell.execute_reply.started":"2022-11-10T18:08:20.317089Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 124, 124, 32)      896       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 122, 122, 64)      18496     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 120, 120, 128)     73856     \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 60, 60, 128)      0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 60, 60, 128)       0         \n","                                                                 \n"," flatten (Flatten)           (None, 460800)            0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 460800)            0         \n","                                                                 \n"," dense (Dense)               (None, 50)                23040050  \n","                                                                 \n"," dropout_2 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 51        \n","                                                                 \n","=================================================================\n","Total params: 23,133,349\n","Trainable params: 23,133,349\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The line code below is for compiling the model by setting the loss to binary cross entropy, the optimizer to adam, and lastly, the metrics to accuracy."]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.327719Z","iopub.status.busy":"2022-11-10T18:08:20.326985Z","iopub.status.idle":"2022-11-10T18:08:20.347258Z","shell.execute_reply":"2022-11-10T18:08:20.346157Z","shell.execute_reply.started":"2022-11-10T18:08:20.327676Z"},"trusted":true},"outputs":[],"source":["model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, to get the xtrain, xval, ytrain, and yval, we have to use the train_test_split."]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.349688Z","iopub.status.busy":"2022-11-10T18:08:20.349075Z","iopub.status.idle":"2022-11-10T18:08:21.189563Z","shell.execute_reply":"2022-11-10T18:08:21.188314Z","shell.execute_reply.started":"2022-11-10T18:08:20.349638Z"},"trusted":true},"outputs":[],"source":["xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, to generate the image data we have to use the image data generator. This is also to fit the xtrain to the data generation variable."]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:21.191528Z","iopub.status.busy":"2022-11-10T18:08:21.191192Z","iopub.status.idle":"2022-11-10T18:08:22.015653Z","shell.execute_reply":"2022-11-10T18:08:22.01426Z","shell.execute_reply.started":"2022-11-10T18:08:21.191497Z"},"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(\n","        featurewise_center=False,  \n","        samplewise_center=False,  \n","        featurewise_std_normalization=False,  \n","        samplewise_std_normalization=False,  \n","        zca_whitening=False,    \n","        rotation_range=15,    \n","        width_shift_range=0.1,\n","        height_shift_range=0.1,  \n","        horizontal_flip=True,  \n","        vertical_flip=False)\n","datagen.fit(xtrain)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now for the history variable, we have to store here the generated fit model along with 50 epochs which may take a while to load."]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:22.017249Z","iopub.status.busy":"2022-11-10T18:08:22.016907Z","iopub.status.idle":"2022-11-11T01:05:41.467431Z","shell.execute_reply":"2022-11-11T01:05:41.46649Z","shell.execute_reply.started":"2022-11-10T18:08:22.017217Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/saraiva/anaconda3/envs/daa/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  \"\"\"\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","  9/143 [>.............................] - ETA: 5:58 - loss: 0.7549 - accuracy: 0.6736"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_71743/236293644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data=(xval, yval))\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2519\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2522\u001b[0m         )\n\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2497\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1863\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n","                    steps_per_epoch=xtrain.shape[0]//32,\n","                    epochs=50,\n","                    verbose=1,\n","                    validation_data=(xval, yval))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Creating Visualisations for Training and Validation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below creates a graph that differentiates the accuracy of training and validation of all the epochs that were created in the history variable."]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.469402Z","iopub.status.busy":"2022-11-11T01:05:41.469089Z","iopub.status.idle":"2022-11-11T01:05:41.688896Z","shell.execute_reply":"2022-11-11T01:05:41.687552Z","shell.execute_reply.started":"2022-11-11T01:05:41.469374Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'history' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_71743/4085118563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Accuracy vs Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]}],"source":["plt.plot(history['accuracy'],'g')\n","plt.plot(history['val_accuracy'],'b')\n","plt.title('Training Accuracy vs Validation Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, the lines of code below creates a graph that differentiates all the epochs' training and validation loss that were also created in the history variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.690895Z","iopub.status.busy":"2022-11-11T01:05:41.690256Z","iopub.status.idle":"2022-11-11T01:05:41.91495Z","shell.execute_reply":"2022-11-11T01:05:41.913828Z","shell.execute_reply.started":"2022-11-11T01:05:41.690865Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history['loss'],'g')\n","plt.plot(history.history['val_loss'],'b')\n","plt.title('Training Loss vs Validation Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Face Mask Detector Model Testing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now for testing the Face Mask Detector Model, we'll have to use a few images from the dataset to be able to evaluate the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.916506Z","iopub.status.busy":"2022-11-11T01:05:41.916212Z","iopub.status.idle":"2022-11-11T01:05:41.92579Z","shell.execute_reply":"2022-11-11T01:05:41.924706Z","shell.execute_reply.started":"2022-11-11T01:05:41.916479Z"},"trusted":true},"outputs":[],"source":["print(len(df_test[\"name\"]),len(df_test[\"name\"].unique()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test_images array is consisted of image names to be use for testing the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.927411Z","iopub.status.busy":"2022-11-11T01:05:41.927048Z","iopub.status.idle":"2022-11-11T01:05:41.934693Z","shell.execute_reply":"2022-11-11T01:05:41.93387Z","shell.execute_reply.started":"2022-11-11T01:05:41.927379Z"},"trusted":true},"outputs":[],"source":["test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, for presenting the results we have to set the gamma = 2, and then set the presentation size to 3 rows and 2 columns."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.936467Z","iopub.status.busy":"2022-11-11T01:05:41.935831Z","iopub.status.idle":"2022-11-11T01:05:45.640771Z","shell.execute_reply":"2022-11-11T01:05:45.639877Z","shell.execute_reply.started":"2022-11-11T01:05:41.936423Z"},"trusted":true},"outputs":[],"source":["gamma = 2.0\n","fig = plt.figure(figsize = (14,14))\n","rows = 3\n","cols = 2\n","axes = []\n","assign = {'0':'Mask','1':\"No Mask\"}\n","for j,im in enumerate(test_images):\n","    image =  cv2.imread(os.path.join(image_directory,im),1)\n","    image =  adjust_gamma(image, gamma=gamma)\n","    (h, w) = image.shape[:2]\n","    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n","    cvNet.setInput(blob)\n","    detections = cvNet.forward()\n","    for i in range(0, detections.shape[2]):\n","        try:\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","            frame = image[startY:endY, startX:endX]\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > 0.2:\n","                im = cv2.resize(frame,(img_size,img_size))\n","                im = np.array(im)/255.0\n","                im = im.reshape(1,124,124,3)\n","                result = model.predict(im)\n","                if result>0.5:\n","                    label_Y = 1\n","                else:\n","                    label_Y = 0\n","                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n","                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n","        \n","        except:pass\n","    axes.append(fig.add_subplot(rows, cols, j+1))\n","    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Conclusion"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Looking at the results, we can see that the whole system performs well when faces have spatial dominance, such as shown in the images at (1,1), (1,2), and (2,1). But it can also be seen that the model fails to detect small faces and take up less space in the overall image which can be seen in (2,2). For better results, you can use 3 different methods such as using the different image preprocessing techniques, or keep the confidence threshold low, or by trying different blob sizes. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"}},"nbformat":4,"nbformat_minor":4}
