{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing the needed Libraries & Directories"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing the required libraries and directories are important to avoid errors for it also allows the codes to work perfectly."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:31.820073Z","iopub.status.busy":"2022-11-10T18:05:31.819191Z","iopub.status.idle":"2022-11-10T18:05:39.182591Z","shell.execute_reply":"2022-11-10T18:05:39.181435Z","shell.execute_reply.started":"2022-11-10T18:05:31.819982Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","import cv2\n","import json\n","import os\n","import matplotlib.pyplot as plt\n","import random\n","import seaborn as sns\n","import torch\n","from torch.nn import Sequential\n","from torch.nn import Conv2d, MaxPool2d, BatchNorm2d\n","from sklearn.model_selection import train_test_split\n","directory = \"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\"\n","image_directory = \"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\"\n","df = pd.read_csv(\"./face-mask-detection-dataset/train.csv\")\n","df_test = pd.read_csv(\"./face-mask-detection-dataset/submission.csv\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["#device management \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","print(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing SSD pretrained weights (Caffe Face Detector Model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["SSD or Single Shot Multibox Detector is used for detecting specific object in a given image. This SSD that I'm going to use is already pre-trained, which means although it has aquired some knowledge about its task, it still need to be trained (which I'll do later in the last lines of code for this project). "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.18486Z","iopub.status.busy":"2022-11-10T18:05:39.18449Z","iopub.status.idle":"2022-11-10T18:05:39.392987Z","shell.execute_reply":"2022-11-10T18:05:39.391805Z","shell.execute_reply.started":"2022-11-10T18:05:39.184826Z"},"trusted":true},"outputs":[],"source":["cvNet = cv2.dnn.readNetFromCaffe('./caffe-face-detector-opencv-pretrained-model/architecture.txt','./caffe-face-detector-opencv-pretrained-model/weights.caffemodel')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Functions to be used"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is the JSON Function. This function retrieves the json file from the training dataset which contains the bounding box data."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.394941Z","iopub.status.busy":"2022-11-10T18:05:39.394585Z","iopub.status.idle":"2022-11-10T18:05:39.400317Z","shell.execute_reply":"2022-11-10T18:05:39.399244Z","shell.execute_reply.started":"2022-11-10T18:05:39.394904Z"},"trusted":true},"outputs":[],"source":["def getJSON(filePathandName):\n","    with open(filePathandName,'r') as f:\n","        return json.load(f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next function is the Gamma correction. When applied, the images will get extra bright. For instance,  gamma < 1 will cause the image to get darker, and gamma > 1 will cause the image to be brighter."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.403892Z","iopub.status.busy":"2022-11-10T18:05:39.402625Z","iopub.status.idle":"2022-11-10T18:05:39.413171Z","shell.execute_reply":"2022-11-10T18:05:39.412088Z","shell.execute_reply.started":"2022-11-10T18:05:39.403854Z"},"trusted":true},"outputs":[],"source":["def adjust_gamma(image, gamma=1.0):\n","    invGamma = 1.0 / gamma\n","    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n","    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Pre-processing the Data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Data pre-processing is used for converting a raw data into a clean dataset.\n","Lets look at the JSON data given for training:\n","\n","The Annotations Field contains all the information about the faces shown in a given image. There are different classnames but the only true classnames are face_with_mask and face_no_mask."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.415287Z","iopub.status.busy":"2022-11-10T18:05:39.414156Z","iopub.status.idle":"2022-11-10T18:06:04.153253Z","shell.execute_reply":"2022-11-10T18:06:04.152043Z","shell.execute_reply.started":"2022-11-10T18:05:39.41525Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'FileName': '2310.png',\n"," 'NumOfAnno': 3,\n"," 'Annotations': [{'isProtected': False,\n","   'ID': 81768340414106304,\n","   'BoundingBox': [213, 169, 325, 299],\n","   'classname': 'face_other_covering',\n","   'Confidence': 1,\n","   'Attributes': {}},\n","  {'isProtected': False,\n","   'ID': 613664631614999424,\n","   'BoundingBox': [171, 168, 346, 299],\n","   'classname': 'hijab_niqab',\n","   'Confidence': 1,\n","   'Attributes': {}},\n","  {'isProtected': False,\n","   'ID': 166231871773593440,\n","   'BoundingBox': [435, 183, 470, 223],\n","   'classname': 'face_no_mask',\n","   'Confidence': 1,\n","   'Attributes': {}}]}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["jsonfiles= []\n","for i in os.listdir(directory):\n","    jsonfiles.append(getJSON(os.path.join(directory,i)))\n","jsonfiles[0]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:06:04.15565Z","iopub.status.busy":"2022-11-10T18:06:04.154997Z","iopub.status.idle":"2022-11-10T18:06:04.1912Z","shell.execute_reply":"2022-11-10T18:06:04.190419Z","shell.execute_reply.started":"2022-11-10T18:06:04.1556Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>y1</th>\n","      <th>y2</th>\n","      <th>classname</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2756.png</td>\n","      <td>69</td>\n","      <td>126</td>\n","      <td>294</td>\n","      <td>392</td>\n","      <td>face_with_mask</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2756.png</td>\n","      <td>505</td>\n","      <td>10</td>\n","      <td>723</td>\n","      <td>283</td>\n","      <td>face_with_mask</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2756.png</td>\n","      <td>75</td>\n","      <td>252</td>\n","      <td>264</td>\n","      <td>390</td>\n","      <td>mask_colorful</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2756.png</td>\n","      <td>521</td>\n","      <td>136</td>\n","      <td>711</td>\n","      <td>277</td>\n","      <td>mask_colorful</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6098.jpg</td>\n","      <td>360</td>\n","      <td>85</td>\n","      <td>728</td>\n","      <td>653</td>\n","      <td>face_no_mask</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       name   x1   x2   y1   y2       classname\n","0  2756.png   69  126  294  392  face_with_mask\n","1  2756.png  505   10  723  283  face_with_mask\n","2  2756.png   75  252  264  390   mask_colorful\n","3  2756.png  521  136  711  277   mask_colorful\n","4  6098.jpg  360   85  728  653    face_no_mask"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['face_with_mask' 'mask_colorful' 'face_no_mask'\n"," 'face_with_mask_incorrect' 'mask_surgical' 'face_other_covering'\n"," 'scarf_bandana' 'eyeglasses' 'helmet' 'face_shield' 'sunglasses' 'hood'\n"," 'hat' 'goggles' 'hair_net' 'hijab_niqab' 'other' 'gas_mask'\n"," 'balaclava_ski_mask' 'turban']\n"]}],"source":["print(df[\"classname\"].unique())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The mask label and non-mask label are utilized to extract bounding box information from json files. The very purpose of the Training Process is to extract and save the label and the faces of the given image into the data list."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:06:04.193107Z","iopub.status.busy":"2022-11-10T18:06:04.192595Z","iopub.status.idle":"2022-11-10T18:08:18.652631Z","shell.execute_reply":"2022-11-10T18:08:18.65157Z","shell.execute_reply.started":"2022-11-10T18:06:04.193068Z"},"trusted":true},"outputs":[],"source":["data = []\n","img_size = 124\n","mask = ['face_with_mask']\n","non_mask = [\"face_no_mask\"]\n","labels={'mask':0,'without mask':1}\n","for i in df[\"name\"].unique():\n","    f = i+\".json\"\n","    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n","        if j[\"classname\"] in mask:\n","            x,y,w,h = j[\"BoundingBox\"]\n","            img = cv2.imread(os.path.join(image_directory,i),1)\n","            img = img[y:h,x:w]\n","            img = cv2.resize(img,(img_size,img_size))\n","            data.append([img,labels[\"mask\"]])\n","        if j[\"classname\"] in non_mask:\n","            x,y,w,h = j[\"BoundingBox\"]\n","            img = cv2.imread(os.path.join(image_directory,i),1)\n","            img = img[y:h,x:w]\n","            img = cv2.resize(img,(img_size,img_size))    \n","            data.append([img,labels[\"without mask\"]])\n","random.shuffle(data)  \n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Checking how many objects are available in the given data."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.654365Z","iopub.status.busy":"2022-11-10T18:08:18.653954Z","iopub.status.idle":"2022-11-10T18:08:18.662006Z","shell.execute_reply":"2022-11-10T18:08:18.660901Z","shell.execute_reply.started":"2022-11-10T18:08:18.654325Z"},"trusted":true},"outputs":[{"data":{"text/plain":["5749"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Base on the visualization below, the Number of Mask images is greater than the Number of Non-Mask images which means its an imbalanced dataset."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.664249Z","iopub.status.busy":"2022-11-10T18:08:18.663346Z","iopub.status.idle":"2022-11-10T18:08:18.915775Z","shell.execute_reply":"2022-11-10T18:08:18.914884Z","shell.execute_reply.started":"2022-11-10T18:08:18.664215Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<AxesSubplot:ylabel='count'>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["p = []\n","for face in data:\n","    if(face[1] == 0):\n","        p.append(\"Mask\")\n","    else:\n","        p.append(\"No Mask\")\n","\n","sns.countplot(x=p)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is used for getting the shape of the features in the face mask data."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.919893Z","iopub.status.busy":"2022-11-10T18:08:18.91904Z","iopub.status.idle":"2022-11-10T18:08:18.92802Z","shell.execute_reply":"2022-11-10T18:08:18.926856Z","shell.execute_reply.started":"2022-11-10T18:08:18.919858Z"},"trusted":true},"outputs":[],"source":["X = []\n","Y = []\n","for features,label in data:\n","    X.append(features)\n","    Y.append(label)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.929811Z","iopub.status.busy":"2022-11-10T18:08:18.92941Z","iopub.status.idle":"2022-11-10T18:08:18.944337Z","shell.execute_reply":"2022-11-10T18:08:18.943131Z","shell.execute_reply.started":"2022-11-10T18:08:18.92974Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(124, 124, 3)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["X[0].shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is used for getting the labels in the face mask data."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.946519Z","iopub.status.busy":"2022-11-10T18:08:18.94586Z","iopub.status.idle":"2022-11-10T18:08:19.979438Z","shell.execute_reply":"2022-11-10T18:08:19.978212Z","shell.execute_reply.started":"2022-11-10T18:08:18.946478Z"},"trusted":true},"outputs":[],"source":["X = np.array(X)/255.0\n","X = X.reshape(-1,124,124,3)\n","Y = np.array(Y)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:19.98116Z","iopub.status.busy":"2022-11-10T18:08:19.980815Z","iopub.status.idle":"2022-11-10T18:08:19.988242Z","shell.execute_reply":"2022-11-10T18:08:19.98718Z","shell.execute_reply.started":"2022-11-10T18:08:19.981129Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(Y)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:19.990381Z","iopub.status.busy":"2022-11-10T18:08:19.989825Z","iopub.status.idle":"2022-11-10T18:08:20.003822Z","shell.execute_reply":"2022-11-10T18:08:20.002505Z","shell.execute_reply.started":"2022-11-10T18:08:19.990349Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5749,)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["Y.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Understanding the Model's Architecture and Training Process"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The project's model architecture is only a rough abstraction. To start the training process we have to set the model to sequential first, after that we can start adding the needed attributes to the model."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.006541Z","iopub.status.busy":"2022-11-10T18:08:20.005541Z","iopub.status.idle":"2022-11-10T18:08:20.315552Z","shell.execute_reply":"2022-11-10T18:08:20.31455Z","shell.execute_reply.started":"2022-11-10T18:08:20.006509Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.flatten = nn.Flatten()\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128 * 30 * 30, 50)\n","        self.dropout3 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(50, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = x.float()\n","        x = nn.ReLU()(self.conv1(x))\n","        x = nn.ReLU()(self.conv2(x))\n","        x = nn.ReLU()(self.conv3(x))\n","        x = self.maxpool(x)\n","        x = self.dropout1(x)\n","        x = self.flatten(x)\n","        x = self.dropout2(x)\n","        x = nn.ReLU()(self.fc1(x))\n","        x = self.dropout3(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","model = Model()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The line code below summarizes the model"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.317122Z","iopub.status.busy":"2022-11-10T18:08:20.316755Z","iopub.status.idle":"2022-11-10T18:08:20.3254Z","shell.execute_reply":"2022-11-10T18:08:20.323692Z","shell.execute_reply.started":"2022-11-10T18:08:20.317089Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Model(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout1): Dropout2d(p=0.25, inplace=False)\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (dropout2): Dropout(p=0.5, inplace=False)\n","  (fc1): Linear(in_features=115200, out_features=50, bias=True)\n","  (dropout3): Dropout(p=0.5, inplace=False)\n","  (fc2): Linear(in_features=50, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["#model.summary()\n","#print(summary(model, input_size=(BATCH_SIZE, 1,28,28), verbose=0))\n","model.to(device) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The line code below is for compiling the model by setting the loss to binary cross entropy, the optimizer to adam, and lastly, the metrics to accuracy."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.327719Z","iopub.status.busy":"2022-11-10T18:08:20.326985Z","iopub.status.idle":"2022-11-10T18:08:20.347258Z","shell.execute_reply":"2022-11-10T18:08:20.346157Z","shell.execute_reply.started":"2022-11-10T18:08:20.327676Z"},"trusted":true},"outputs":[],"source":["#model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n","from livelossplot import PlotLosses\n","\n","\n","def train_model(h5_file,train_dl, test_dl, model, loss_function,optimizer, scheduler, epochs):\n","    liveloss = PlotLosses()\n","    for epoch in range(epochs):\n","        logs = {}\n","        model.train()\n","        running_loss = 0.0\n","        for _, (inputs, _) in enumerate(train_dl):\n","            inputs = inputs.to(device)\n","            print(inputs.shape)\n","            outputs,_ = model(inputs)\n","            loss = loss_function(outputs, inputs)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        epoch_loss = running_loss / len(train_dl.dataset)\n","        logs['loss'] = epoch_loss*1000\n","        #Validation phase\n","        model.eval()\n","        running_loss = 0.0\n","        for inputs, labels in test_dl:\n","            inputs = inputs.to(device)\n","            outputs,_ = model(inputs)\n","            loss = loss_function(outputs, inputs)\n","            running_loss += loss.item()\n","        epoch_loss = running_loss / len(test_dl.dataset)\n","        logs['val_loss'] = epoch_loss*1000\n","        scheduler.step(epoch_loss) #callback a meio para atualizar lr\n","        epoch_lr = optimizer.param_groups[0]['lr']\n","        logs['val_lr'] = epoch_lr\n","        liveloss.update(logs) #para visualizarmos o processo de treino\n","        liveloss.send() #para visualizarmos o processo de treino\n","    torch.save(model,h5_file) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, to get the xtrain, xval, ytrain, and yval, we have to use the train_test_split."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.349688Z","iopub.status.busy":"2022-11-10T18:08:20.349075Z","iopub.status.idle":"2022-11-10T18:08:21.189563Z","shell.execute_reply":"2022-11-10T18:08:21.188314Z","shell.execute_reply.started":"2022-11-10T18:08:20.349638Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4599, 124, 124, 3])\n","torch.Size([1150, 124, 124, 3])\n","torch.Size([4599, 3, 124, 124])\n","torch.Size([1150, 3, 124, 124])\n"]}],"source":["\n","\n","\n","xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","\n","\n","tensor1 = torch.tensor(xtrain)\n","tensor2 = torch.tensor(xtest)\n","\n","\n","print(tensor1.shape)\n","print(tensor2.shape)\n","\n","tensor1 = tensor1.permute(0,3,1,2)\n","tensor2 = tensor2.permute(0,3,1,2)\n","\n","train = torch.utils.data.TensorDataset( tensor1, torch.tensor(ytrain))\n","test = torch.utils.data.TensorDataset(tensor2, torch.tensor(ytest))\n","\n","print(tensor1.shape)\n","print(tensor2.shape)\n","\n","\n","\n","\n","\n","train_dl = DataLoader(train, batch_size=32, shuffle=True)\n","test_dl = DataLoader(test, batch_size=32, shuffle=True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'shpe'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_12035/3087579203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tempo gasto: {endtime - starttime} segundos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_12035/713722347.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(h5_file, train_dl, test_dl, model, loss_function, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'shpe'"]}],"source":["EPOCHS = 50\n","LEARNING_RATE = 0.001\n","\n","from torch.nn import  BCELoss\n","from torch.optim import Adam \n","from torch.optim.lr_scheduler import StepLR\n","import time\n","from livelossplot import PlotLosses\n","\n","# definir o loss e a função de otimização\n","loss_function = BCELoss()\n","optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n","scheduler= StepLR(optimizer,step_size=10,gamma=0.95)\n","starttime = time.perf_counter()\n","train_model('./model.pth', train, test, model, loss_function, optimizer, scheduler, EPOCHS)\n","endtime = time.perf_counter()\n","print(f\"Tempo gasto: {endtime - starttime} segundos\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, to generate the image data we have to use the image data generator. This is also to fit the xtrain to the data generation variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:21.191528Z","iopub.status.busy":"2022-11-10T18:08:21.191192Z","iopub.status.idle":"2022-11-10T18:08:22.015653Z","shell.execute_reply":"2022-11-10T18:08:22.01426Z","shell.execute_reply.started":"2022-11-10T18:08:21.191497Z"},"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(\n","        featurewise_center=False,  \n","        samplewise_center=False,  \n","        featurewise_std_normalization=False,  \n","        samplewise_std_normalization=False,  \n","        zca_whitening=False,    \n","        rotation_range=15,    \n","        width_shift_range=0.1,\n","        height_shift_range=0.1,  \n","        horizontal_flip=True,  \n","        vertical_flip=False)\n","datagen.fit(xtrain)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now for the history variable, we have to store here the generated fit model along with 50 epochs which may take a while to load."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:22.017249Z","iopub.status.busy":"2022-11-10T18:08:22.016907Z","iopub.status.idle":"2022-11-11T01:05:41.467431Z","shell.execute_reply":"2022-11-11T01:05:41.46649Z","shell.execute_reply.started":"2022-11-10T18:08:22.017217Z"},"trusted":true},"outputs":[],"source":["history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n","                    steps_per_epoch=xtrain.shape[0]//32,\n","                    epochs=50,\n","                    verbose=1,\n","                    validation_data=(xval, yval))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Creating Visualisations for Training and Validation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below creates a graph that differentiates the accuracy of training and validation of all the epochs that were created in the history variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.469402Z","iopub.status.busy":"2022-11-11T01:05:41.469089Z","iopub.status.idle":"2022-11-11T01:05:41.688896Z","shell.execute_reply":"2022-11-11T01:05:41.687552Z","shell.execute_reply.started":"2022-11-11T01:05:41.469374Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history['accuracy'],'g')\n","plt.plot(history.history['val_accuracy'],'b')\n","plt.title('Training Accuracy vs Validation Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, the lines of code below creates a graph that differentiates all the epochs' training and validation loss that were also created in the history variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.690895Z","iopub.status.busy":"2022-11-11T01:05:41.690256Z","iopub.status.idle":"2022-11-11T01:05:41.91495Z","shell.execute_reply":"2022-11-11T01:05:41.913828Z","shell.execute_reply.started":"2022-11-11T01:05:41.690865Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history['loss'],'g')\n","plt.plot(history.history['val_loss'],'b')\n","plt.title('Training Loss vs Validation Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Face Mask Detector Model Testing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now for testing the Face Mask Detector Model, we'll have to use a few images from the dataset to be able to evaluate the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.916506Z","iopub.status.busy":"2022-11-11T01:05:41.916212Z","iopub.status.idle":"2022-11-11T01:05:41.92579Z","shell.execute_reply":"2022-11-11T01:05:41.924706Z","shell.execute_reply.started":"2022-11-11T01:05:41.916479Z"},"trusted":true},"outputs":[],"source":["print(len(df_test[\"name\"]),len(df_test[\"name\"].unique()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test_images array is consisted of image names to be use for testing the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.927411Z","iopub.status.busy":"2022-11-11T01:05:41.927048Z","iopub.status.idle":"2022-11-11T01:05:41.934693Z","shell.execute_reply":"2022-11-11T01:05:41.93387Z","shell.execute_reply.started":"2022-11-11T01:05:41.927379Z"},"trusted":true},"outputs":[],"source":["test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, for presenting the results we have to set the gamma = 2, and then set the presentation size to 3 rows and 2 columns."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.936467Z","iopub.status.busy":"2022-11-11T01:05:41.935831Z","iopub.status.idle":"2022-11-11T01:05:45.640771Z","shell.execute_reply":"2022-11-11T01:05:45.639877Z","shell.execute_reply.started":"2022-11-11T01:05:41.936423Z"},"trusted":true},"outputs":[],"source":["gamma = 2.0\n","fig = plt.figure(figsize = (14,14))\n","rows = 3\n","cols = 2\n","axes = []\n","assign = {'0':'Mask','1':\"No Mask\"}\n","for j,im in enumerate(test_images):\n","    image =  cv2.imread(os.path.join(image_directory,im),1)\n","    image =  adjust_gamma(image, gamma=gamma)\n","    (h, w) = image.shape[:2]\n","    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n","    cvNet.setInput(blob)\n","    detections = cvNet.forward()\n","    for i in range(0, detections.shape[2]):\n","        try:\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","            frame = image[startY:endY, startX:endX]\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > 0.2:\n","                im = cv2.resize(frame,(img_size,img_size))\n","                im = np.array(im)/255.0\n","                im = im.reshape(1,124,124,3)\n","                result = model.predict(im)\n","                if result>0.5:\n","                    label_Y = 1\n","                else:\n","                    label_Y = 0\n","                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n","                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n","        \n","        except:pass\n","    axes.append(fig.add_subplot(rows, cols, j+1))\n","    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Conclusion"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Looking at the results, we can see that the whole system performs well when faces have spatial dominance, such as shown in the images at (1,1), (1,2), and (2,1). But it can also be seen that the model fails to detect small faces and take up less space in the overall image which can be seen in (2,2). For better results, you can use 3 different methods such as using the different image preprocessing techniques, or keep the confidence threshold low, or by trying different blob sizes. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"}},"nbformat":4,"nbformat_minor":4}
