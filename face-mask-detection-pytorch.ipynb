{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing the needed Libraries & Directories"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing the required libraries and directories are important to avoid errors for it also allows the codes to work perfectly."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:31.820073Z","iopub.status.busy":"2022-11-10T18:05:31.819191Z","iopub.status.idle":"2022-11-10T18:05:39.182591Z","shell.execute_reply":"2022-11-10T18:05:39.181435Z","shell.execute_reply.started":"2022-11-10T18:05:31.819982Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/face-mask-detection-dataset/submission.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_71743/4053074468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimage_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./face-mask-detection-dataset/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/face-mask-detection-dataset/submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/daa/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/face-mask-detection-dataset/submission.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import json\n","import os\n","import matplotlib.pyplot as plt\n","import random\n","import seaborn as sns\n","from keras.models import Sequential\n","from keras import optimizers\n","from keras import backend as K\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator\n","directory = \"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\"\n","image_directory = \"./face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\"\n","df = pd.read_csv(\"./face-mask-detection-dataset/train.csv\")\n","df_test = pd.read_csv(\"../input/face-mask-detection-dataset/submission.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing SSD pretrained weights (Caffe Face Detector Model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["SSD or Single Shot Multibox Detector is used for detecting specific object in a given image. This SSD that I'm going to use is already pre-trained, which means although it has aquired some knowledge about its task, it still need to be trained (which I'll do later in the last lines of code for this project). "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.18486Z","iopub.status.busy":"2022-11-10T18:05:39.18449Z","iopub.status.idle":"2022-11-10T18:05:39.392987Z","shell.execute_reply":"2022-11-10T18:05:39.391805Z","shell.execute_reply.started":"2022-11-10T18:05:39.184826Z"},"trusted":true},"outputs":[],"source":["cvNet = cv2.dnn.readNetFromCaffe('../input/caffe-face-detector-opencv-pretrained-model/architecture.txt','../input/caffe-face-detector-opencv-pretrained-model/weights.caffemodel')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Functions to be used"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is the JSON Function. This function retrieves the json file from the training dataset which contains the bounding box data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.394941Z","iopub.status.busy":"2022-11-10T18:05:39.394585Z","iopub.status.idle":"2022-11-10T18:05:39.400317Z","shell.execute_reply":"2022-11-10T18:05:39.399244Z","shell.execute_reply.started":"2022-11-10T18:05:39.394904Z"},"trusted":true},"outputs":[],"source":["def getJSON(filePathandName):\n","    with open(filePathandName,'r') as f:\n","        return json.load(f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next function is the Gamma correction. When applied, the images will get extra bright. For instance,  gamma < 1 will cause the image to get darker, and gamma > 1 will cause the image to be brighter."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.403892Z","iopub.status.busy":"2022-11-10T18:05:39.402625Z","iopub.status.idle":"2022-11-10T18:05:39.413171Z","shell.execute_reply":"2022-11-10T18:05:39.412088Z","shell.execute_reply.started":"2022-11-10T18:05:39.403854Z"},"trusted":true},"outputs":[],"source":["def adjust_gamma(image, gamma=1.0):\n","    invGamma = 1.0 / gamma\n","    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n","    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Pre-processing the Data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Data pre-processing is used for converting a raw data into a clean dataset.\n","Lets look at the JSON data given for training:\n","\n","The Annotations Field contains all the information about the faces shown in a given image. There are different classnames but the only true classnames are face_with_mask and face_no_mask."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:05:39.415287Z","iopub.status.busy":"2022-11-10T18:05:39.414156Z","iopub.status.idle":"2022-11-10T18:06:04.153253Z","shell.execute_reply":"2022-11-10T18:06:04.152043Z","shell.execute_reply.started":"2022-11-10T18:05:39.41525Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_71743/3371778965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjsonfiles\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mjsonfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjsonfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["jsonfiles= []\n","for i in os.listdir(directory):\n","    jsonfiles.append(getJSON(os.path.join(directory,i)))\n","jsonfiles[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:06:04.15565Z","iopub.status.busy":"2022-11-10T18:06:04.154997Z","iopub.status.idle":"2022-11-10T18:06:04.1912Z","shell.execute_reply":"2022-11-10T18:06:04.190419Z","shell.execute_reply.started":"2022-11-10T18:06:04.1556Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"../input/face-mask-detection-dataset/train.csv\")\n","df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The mask label and non-mask label are utilized to extract bounding box information from json files. The very purpose of the Training Process is to extract and save the label and the faces of the given image into the data list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:06:04.193107Z","iopub.status.busy":"2022-11-10T18:06:04.192595Z","iopub.status.idle":"2022-11-10T18:08:18.652631Z","shell.execute_reply":"2022-11-10T18:08:18.65157Z","shell.execute_reply.started":"2022-11-10T18:06:04.193068Z"},"trusted":true},"outputs":[],"source":["data = []\n","img_size = 124\n","mask = ['face_with_mask']\n","non_mask = [\"face_no_mask\"]\n","labels={'mask':0,'without mask':1}\n","for i in df[\"name\"].unique():\n","    f = i+\".json\"\n","    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n","        if j[\"classname\"] in mask:\n","            x,y,w,h = j[\"BoundingBox\"]\n","            img = cv2.imread(os.path.join(image_directory,i),1)\n","            img = img[y:h,x:w]\n","            img = cv2.resize(img,(img_size,img_size))\n","            data.append([img,labels[\"mask\"]])\n","        if j[\"classname\"] in non_mask:\n","            x,y,w,h = j[\"BoundingBox\"]\n","            img = cv2.imread(os.path.join(image_directory,i),1)\n","            img = img[y:h,x:w]\n","            img = cv2.resize(img,(img_size,img_size))    \n","            data.append([img,labels[\"without mask\"]])\n","random.shuffle(data)    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Checking how many objects are available in the given data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.654365Z","iopub.status.busy":"2022-11-10T18:08:18.653954Z","iopub.status.idle":"2022-11-10T18:08:18.662006Z","shell.execute_reply":"2022-11-10T18:08:18.660901Z","shell.execute_reply.started":"2022-11-10T18:08:18.654325Z"},"trusted":true},"outputs":[],"source":["len(data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Base on the visualization below, the Number of Mask images is greater than the Number of Non-Mask images which means its an imbalanced dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.664249Z","iopub.status.busy":"2022-11-10T18:08:18.663346Z","iopub.status.idle":"2022-11-10T18:08:18.915775Z","shell.execute_reply":"2022-11-10T18:08:18.914884Z","shell.execute_reply.started":"2022-11-10T18:08:18.664215Z"},"trusted":true},"outputs":[],"source":["p = []\n","for face in data:\n","    if(face[1] == 0):\n","        p.append(\"Mask\")\n","    else:\n","        p.append(\"No Mask\")\n","sns.countplot(p)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is used for getting the shape of the features in the face mask data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.919893Z","iopub.status.busy":"2022-11-10T18:08:18.91904Z","iopub.status.idle":"2022-11-10T18:08:18.92802Z","shell.execute_reply":"2022-11-10T18:08:18.926856Z","shell.execute_reply.started":"2022-11-10T18:08:18.919858Z"},"trusted":true},"outputs":[],"source":["X = []\n","Y = []\n","for features,label in data:\n","    X.append(features)\n","    Y.append(label)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.929811Z","iopub.status.busy":"2022-11-10T18:08:18.92941Z","iopub.status.idle":"2022-11-10T18:08:18.944337Z","shell.execute_reply":"2022-11-10T18:08:18.943131Z","shell.execute_reply.started":"2022-11-10T18:08:18.92974Z"},"trusted":true},"outputs":[],"source":["X[0].shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below is used for getting the labels in the face mask data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:18.946519Z","iopub.status.busy":"2022-11-10T18:08:18.94586Z","iopub.status.idle":"2022-11-10T18:08:19.979438Z","shell.execute_reply":"2022-11-10T18:08:19.978212Z","shell.execute_reply.started":"2022-11-10T18:08:18.946478Z"},"trusted":true},"outputs":[],"source":["X = np.array(X)/255.0\n","X = X.reshape(-1,124,124,3)\n","Y = np.array(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:19.98116Z","iopub.status.busy":"2022-11-10T18:08:19.980815Z","iopub.status.idle":"2022-11-10T18:08:19.988242Z","shell.execute_reply":"2022-11-10T18:08:19.98718Z","shell.execute_reply.started":"2022-11-10T18:08:19.981129Z"},"trusted":true},"outputs":[],"source":["np.unique(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:19.990381Z","iopub.status.busy":"2022-11-10T18:08:19.989825Z","iopub.status.idle":"2022-11-10T18:08:20.003822Z","shell.execute_reply":"2022-11-10T18:08:20.002505Z","shell.execute_reply.started":"2022-11-10T18:08:19.990349Z"},"trusted":true},"outputs":[],"source":["Y.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Understanding the Model's Architecture and Training Process"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The project's model architecture is only a rough abstraction. To start the training process we have to set the model to sequential first, after that we can start adding the needed attributes to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.006541Z","iopub.status.busy":"2022-11-10T18:08:20.005541Z","iopub.status.idle":"2022-11-10T18:08:20.315552Z","shell.execute_reply":"2022-11-10T18:08:20.31455Z","shell.execute_reply.started":"2022-11-10T18:08:20.006509Z"},"trusted":true},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n"," \n","model.add(Flatten())\n","model.add(Dropout(0.5))\n","model.add(Dense(50, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The line code below summarizes the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.317122Z","iopub.status.busy":"2022-11-10T18:08:20.316755Z","iopub.status.idle":"2022-11-10T18:08:20.3254Z","shell.execute_reply":"2022-11-10T18:08:20.323692Z","shell.execute_reply.started":"2022-11-10T18:08:20.317089Z"},"trusted":true},"outputs":[],"source":["model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The line code below is for compiling the model by setting the loss to binary cross entropy, the optimizer to adam, and lastly, the metrics to accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.327719Z","iopub.status.busy":"2022-11-10T18:08:20.326985Z","iopub.status.idle":"2022-11-10T18:08:20.347258Z","shell.execute_reply":"2022-11-10T18:08:20.346157Z","shell.execute_reply.started":"2022-11-10T18:08:20.327676Z"},"trusted":true},"outputs":[],"source":["model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, to get the xtrain, xval, ytrain, and yval, we have to use the train_test_split."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:20.349688Z","iopub.status.busy":"2022-11-10T18:08:20.349075Z","iopub.status.idle":"2022-11-10T18:08:21.189563Z","shell.execute_reply":"2022-11-10T18:08:21.188314Z","shell.execute_reply.started":"2022-11-10T18:08:20.349638Z"},"trusted":true},"outputs":[],"source":["xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, to generate the image data we have to use the image data generator. This is also to fit the xtrain to the data generation variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:21.191528Z","iopub.status.busy":"2022-11-10T18:08:21.191192Z","iopub.status.idle":"2022-11-10T18:08:22.015653Z","shell.execute_reply":"2022-11-10T18:08:22.01426Z","shell.execute_reply.started":"2022-11-10T18:08:21.191497Z"},"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(\n","        featurewise_center=False,  \n","        samplewise_center=False,  \n","        featurewise_std_normalization=False,  \n","        samplewise_std_normalization=False,  \n","        zca_whitening=False,    \n","        rotation_range=15,    \n","        width_shift_range=0.1,\n","        height_shift_range=0.1,  \n","        horizontal_flip=True,  \n","        vertical_flip=False)\n","datagen.fit(xtrain)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now for the history variable, we have to store here the generated fit model along with 50 epochs which may take a while to load."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-10T18:08:22.017249Z","iopub.status.busy":"2022-11-10T18:08:22.016907Z","iopub.status.idle":"2022-11-11T01:05:41.467431Z","shell.execute_reply":"2022-11-11T01:05:41.46649Z","shell.execute_reply.started":"2022-11-10T18:08:22.017217Z"},"trusted":true},"outputs":[],"source":["history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n","                    steps_per_epoch=xtrain.shape[0]//32,\n","                    epochs=50,\n","                    verbose=1,\n","                    validation_data=(xval, yval))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Creating Visualisations for Training and Validation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The lines of code below creates a graph that differentiates the accuracy of training and validation of all the epochs that were created in the history variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.469402Z","iopub.status.busy":"2022-11-11T01:05:41.469089Z","iopub.status.idle":"2022-11-11T01:05:41.688896Z","shell.execute_reply":"2022-11-11T01:05:41.687552Z","shell.execute_reply.started":"2022-11-11T01:05:41.469374Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history['accuracy'],'g')\n","plt.plot(history.history['val_accuracy'],'b')\n","plt.title('Training Accuracy vs Validation Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, the lines of code below creates a graph that differentiates all the epochs' training and validation loss that were also created in the history variable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.690895Z","iopub.status.busy":"2022-11-11T01:05:41.690256Z","iopub.status.idle":"2022-11-11T01:05:41.91495Z","shell.execute_reply":"2022-11-11T01:05:41.913828Z","shell.execute_reply.started":"2022-11-11T01:05:41.690865Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history['loss'],'g')\n","plt.plot(history.history['val_loss'],'b')\n","plt.title('Training Loss vs Validation Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Face Mask Detector Model Testing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now for testing the Face Mask Detector Model, we'll have to use a few images from the dataset to be able to evaluate the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.916506Z","iopub.status.busy":"2022-11-11T01:05:41.916212Z","iopub.status.idle":"2022-11-11T01:05:41.92579Z","shell.execute_reply":"2022-11-11T01:05:41.924706Z","shell.execute_reply.started":"2022-11-11T01:05:41.916479Z"},"trusted":true},"outputs":[],"source":["print(len(df_test[\"name\"]),len(df_test[\"name\"].unique()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test_images array is consisted of image names to be use for testing the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.927411Z","iopub.status.busy":"2022-11-11T01:05:41.927048Z","iopub.status.idle":"2022-11-11T01:05:41.934693Z","shell.execute_reply":"2022-11-11T01:05:41.93387Z","shell.execute_reply.started":"2022-11-11T01:05:41.927379Z"},"trusted":true},"outputs":[],"source":["test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, for presenting the results we have to set the gamma = 2, and then set the presentation size to 3 rows and 2 columns."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:05:41.936467Z","iopub.status.busy":"2022-11-11T01:05:41.935831Z","iopub.status.idle":"2022-11-11T01:05:45.640771Z","shell.execute_reply":"2022-11-11T01:05:45.639877Z","shell.execute_reply.started":"2022-11-11T01:05:41.936423Z"},"trusted":true},"outputs":[],"source":["gamma = 2.0\n","fig = plt.figure(figsize = (14,14))\n","rows = 3\n","cols = 2\n","axes = []\n","assign = {'0':'Mask','1':\"No Mask\"}\n","for j,im in enumerate(test_images):\n","    image =  cv2.imread(os.path.join(image_directory,im),1)\n","    image =  adjust_gamma(image, gamma=gamma)\n","    (h, w) = image.shape[:2]\n","    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n","    cvNet.setInput(blob)\n","    detections = cvNet.forward()\n","    for i in range(0, detections.shape[2]):\n","        try:\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","            frame = image[startY:endY, startX:endX]\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > 0.2:\n","                im = cv2.resize(frame,(img_size,img_size))\n","                im = np.array(im)/255.0\n","                im = im.reshape(1,124,124,3)\n","                result = model.predict(im)\n","                if result>0.5:\n","                    label_Y = 1\n","                else:\n","                    label_Y = 0\n","                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n","                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n","        \n","        except:pass\n","    axes.append(fig.add_subplot(rows, cols, j+1))\n","    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Conclusion"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Looking at the results, we can see that the whole system performs well when faces have spatial dominance, such as shown in the images at (1,1), (1,2), and (2,1). But it can also be seen that the model fails to detect small faces and take up less space in the overall image which can be seen in (2,2). For better results, you can use 3 different methods such as using the different image preprocessing techniques, or keep the confidence threshold low, or by trying different blob sizes. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"}},"nbformat":4,"nbformat_minor":4}
